---
title: Predicting Exercise Effectiveness with Fitness Device Data and Machine Learning
author: "David R Scott"
date: "October 23, 2016"
output: 
  html_document: 
    keep_md: yes
---
**NOTE FOR GRADERS:** 
  The complied .html version can be viewed here:
  <https://belacqua.github.io/practicalmachinelearning/>
  
  And for reference, the path that brought you to the current file in this     repository, which contains the index.Rmd, index.md., and index.html files
  which also contain this report and the code and outcome of this project, is   here:
  <https://github.com/Belacqua/practicalmachinelearning>.
  
  
**Background**

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit; it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. 

These device users regularly  quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 


### Aims of the Project  

The goal of your project is to predict the manner and effectiveness in which these device users did the exercise -- the "classe" variable in the training set. The other variables in the device data will be used to predict with.


### Data  

The training data for this project are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>


### Process, Results  

The process and results of the project include the following items, which are detailed, in order, in the remainder of this report:

* Understanding the prediction question. 
      + The question in this project is, can we identify the when the participants perform barbell        lifts correctly and incorrectly in 5 different ways, from the various metrics recorded 
       on their devices. 
      + Also, it would help to identify some of the key factors in such                predictions.
       
* Getting and cleaning the device data. 
      + This includes not only reading it into R as an R object, but also            dealing with preprocessing to account for skewed or missing data,            creation of dummy variables, and otherwise accommodating the data to           the assumptions of the prediction problem and modeling techniques             available.

* Final Feature Selection. 
      + Analyze data for correlation analysis, and pick those which matter.          There are over 150 features available and many  may reflect                  same information, or contain no relevant information. Here is where
        we choose those which seem to have the most effect on the target.

* Algorithm selection, and building and evaluating the models. 
      + Classification trees are created, along with supporting graphs.         
      + But Random Forests are eventually used, for better performance. The          model is then tested         on the test (held out) data.
      
* Interpreting the final model, and making predictions. 
      + A method to estimate the out of sample error is selected and applied
     
      + The model output should allow interpretation of the results in terms         of the model features, so that the results can be meaningful to the          users.
      
      + New predictions are the main result of the whole process, so the test        data is used to make predictions on 20 test cases.

### Getting and cleaning the device data 

**Begin by reading in the CSV file of training and test data.**
Read the data from the local working directory, and looking at the distribution of the target variable 'classe'. 


```{r, echo = FALSE, cache=TRUE}

#Read in Training and test data sets
setwd("~/Data Science Training/Coursera/Data Science Specialization/Course 8 - Machine Learning/Course Project")
training<-read.csv("pml-training.csv")
testing<-read.csv("pml-testing.csv")
training[1,160] #look at the 'classe' variable, which is to be predicted.
table(training$classe)
library(ggplot2)
qplot(classe,data=training, fill=classe)

```

Note that while 'A' is the mode of the class levels, it represents much less than half the results. So a predictive model is required to get some traction, since guessing 'A' would usually be wrong.

Next, we pre-process the data.

This step includes looking at the structure of the data, using this information for finding and conversion of factor variables to numeric, as well as dropping useless fields, imputing missing values with their nearest neighbor via the k-nearest neighbor algorithm, and creating dummy variables for the 5 levels of the 'classe' variable.

We also center and scale the numeric variables.
```{r, echo = FALSE, cache=TRUE, warning =FALSE}

##########################################################
#Preprocess to center, scale, and imput missing values
##########################################################
#set the seed for all random subsetting
set.seed(1234)

#Separate the classe variable to re-attach to the feature 
#set after more preprocessing, for both train and test data.
classe=data.frame(training[,160])
colnames(classe)=c("classe")

problem.id=data.frame(testing[,160])
colnames(problem.id)=c("problem_id")

#Look at the data structure, to determine needed changes:
#str(training)

#Make certain factors into numeric, where data shows it is 
#appropriate, for trainign and testing data:
indx<- c(12,13,15,16,20,23,69,70,71,72,73,74,87,88,90,91,95,98,125,126,128,129,133,136)

training[indx] <- lapply(training[indx], function(x) as.numeric(as.character(x)))

testing[indx] <- lapply(testing[indx], function(x) as.numeric(as.character(x)))

#Drop numeric data where information appears weak, misleading, 
#spurious, or non-existent:
indx<- c(1,14,17,26,89,92,101,127,130,139) 
training=(training[-indx])
testing=(testing[-indx])

#Center, Scale, and Imput missing values with nearest neighbor:
library(caret)
#Train
preObj <- preProcess(training[,-150],method="knnImpute")
trainProcessed <- predict(preObj,training[,-150]) #drop target var 'classe' 
trainProcessed<-cbind(trainProcessed,classe)#add target back in 
#Test - Use the imputaton model from the training data
testProcessed <- predict(preObj,testing[,-150]) #drop the test problem_id  
test.subset<-cbind(testProcessed,problem.id) #rename data for steps below

#Take small sample of test data for correlation analysis and
#model testing:
trainProcessed$randomNum<-runif(nrow(trainProcessed))
train.subset<-trainProcessed[trainProcessed$randomNum<.1,]

```

### Final Feature Selection.

* Next, after creating dummy variables for the levels of 'classe', we build 
a correlation table to discover which of the newly transformed features are most closely related to the 'classe' dummy variables.
```{r, echo = FALSE, cache=TRUE}
#Create Dummy variables for the target variable 'classe'
library(dummies)
train.subset=cbind(train.subset,dummy(train.subset$classe, data=NULL, sep = "", 
      drop = TRUE, fun = as.integer, verbose = FALSE))

#Calculate correlations between numeric features,
#keeping the non-target correlations as rows, to
#each of the 5 target outcome dummy variables:
indx <- sapply(train.subset, is.numeric)
#trainMatrix<-as.matrix(train.subset[indx])
trainForCorr<-train.subset[indx]
#cor(trainMatrix, use="complete.obs", method="kendall") 
#cov(trainMatrix, use="complete.obs")  
Target.correlations<-data.frame(cor(trainForCorr, use="complete.obs", method="kendall"))
#Limit to non-target rows, and target columns:
Target.correlations=Target.correlations[1:146,148:152]
Target.correlations$Feature=rownames(Target.correlations)
#write.table(Target.correlations,file="TargetCorrelations.csv",row.names=TRUE,sep=",")

```
* Each variable is ranked by the absolute value of its correlation to each of the target dummy variables.
```{r, echo = FALSE, cache=TRUE}
#Top 10 correlated features, ordering by the  
#absolute value of the correlation,so that
#the strongest correlations are captured for
##each of the 5 target categories A-E:
#Classe='A':
ClasseA.corr<-Target.correlations[,c(1,6)]
ClasseA.corr<-Target.correlations[,c(1,6)]
ClasseA.corr$absoluteValueA= abs(ClasseA.corr[,1])
ClasseA.corr=ClasseA.corr[ order(ClasseA.corr[[3]],decreasing=TRUE), ]
ClasseA.corr=ClasseA.corr[1:10,]
#Classe='B':
ClasseB.corr<-Target.correlations[,c(2,6)]
ClasseB.corr$absoluteValueB= abs(ClasseB.corr[,1])
ClasseB.corr=ClasseB.corr[ order(ClasseB.corr[[3]],decreasing=TRUE), ]
ClasseB.corr=ClasseB.corr[1:10,]
#Classe='C':
ClasseC.corr<-Target.correlations[,c(3,6)]
ClasseC.corr$absoluteValueC= abs(ClasseC.corr[,1])
ClasseC.corr=ClasseC.corr[ order(ClasseC.corr[[3]],decreasing=TRUE), ]
ClasseC.corr=ClasseC.corr[1:10,]
#Classe='D':
ClasseD.corr<-Target.correlations[,c(4,6)]
ClasseD.corr$absoluteValueD= abs(ClasseD.corr[,1])
ClasseD.corr=ClasseD.corr[ order(ClasseD.corr[[3]],decreasing=TRUE), ]
ClasseD.corr=ClasseD.corr[1:10,]
#Classe='E':
ClasseE.corr<-Target.correlations[,c(5,6)]
ClasseE.corr$absoluteValueE= abs(ClasseE.corr[,1])
ClasseE.corr=ClasseE.corr[ order(ClasseE.corr[[3]],decreasing=TRUE), ]
ClasseE.corr=ClasseE.corr[1:10,]
```

```{r, echo = FALSE, cache=TRUE}
#Create a single table of all the top 'Classe' correlation variables:
AllClasse.corr <- merge(ClasseA.corr[,2:3], ClasseB.corr[,2:3], by.x = "Feature", by.y = "Feature",all = TRUE)
AllClasse.corr <- merge(AllClasse.corr, ClasseC.corr[,2:3], by.x = "Feature", by.y = "Feature",all = TRUE)
AllClasse.corr <- merge(AllClasse.corr, ClasseD.corr[,2:3], by.x = "Feature", by.y = "Feature",all = TRUE)
AllClasse.corr <- merge(AllClasse.corr, ClasseE.corr[,2:3], by.x = "Feature", by.y = "Feature",all = TRUE)

```
* The results are collated and used as the final model data set.

### Algorithm selection, and building and evaluating the models. 

The first algorithm chosen is a decision tree model. This model is built on the final training data set from the last step above, and evaluated for it's accuracy (SEE TREE CHART BELOW):
```{r, echo = FALSE, cache=TRUE}
##########################################################
#Model Build
##########################################################

#BUILD Decision Tree Model
train.subset2=train.subset[,AllClasse.corr[,1]]
test.subset2=test.subset[,AllClasse.corr[,1]]

train.subset2$classe=train.subset$classe
test.subset2$problem_id=test.subset$problem_id

#Run Recursive Partitioning and Regression Trees Model to predict classe category
modFit <- train(classe~ .,data=train.subset2,method="rpart")

#Plot tree
library(rattle); library(rpart.plot)
fancyRpartPlot(modFit$finalModel)

#predict test cases
#predict(modFit,newdata=test.subset2)

```

While there is clearly some improvement over random guessing with just a few learned classification rules which the model generated, note that the decision tree predicted classes are only correct about half the time, even in the training data: 

(0.98 x 0.09)+(0.33 x 0.54)+(0.59 x 0.09)+(0.57 x 0.09)+(0.55 x 0.2)= **48.1%** 

Part of the problem in the decision tree approach was the lack of any cross-validation in the use of the training data. 

But since this model did not perform well at all, the next step is to improve this modeling approach with a **Random Forest model,** which will create many boostrapped sub-samples of the training data. In addition, this approach will try many subsets of all possible trees, and internally select the 'champion' of all these hidden alternative models. 

As seen below, the random forest results are far more promising than the decision tree results, with an accuracy of over 87% in the training data: 

```{r, echo = FALSE, cache=TRUE}
#Build a Random Forest model
library(caret)
modFit <- train(classe~ .,data=train.subset2,method="rf",prox=TRUE)
print(modFit)

#predict on new data
#predict(modFit,newdata=test.subset2)

#MODEL EVALUATION -- KAPPA is recommended method.

```

### Interpreting the final model, and making predictions. 

Note above that in addition to the the random forest model's 87% training data accuracy in its final iteration, it also has a Kappa coefficient of over .84. This measures the degree that the categories are correct *above what would be expected with random luck*, on a scale of roughly 0 to 1, and is recommended as a method to evaluate non-binary classification predictions.

**Interpreting the 0.84 Magnitude of Kappa**

Landis and Koch have proposed the following as standards for strength of agreement for the kappa coefficient: 

(zero or less) = poor

   .01 –  .20  = slight

   .21 –  .40  = fair

   .41 –  .60  = moderate

   .61 –  .80  = substantial, and 

   .81 – 1.0   = almost perfect. 

Based on this benchmark, the model has performed almost perfectly at disintguishing classes beyond expected random luck. The choice of such benchmarks as 'poor' or 'almost perfect', however, is inevitably arbitrary,and the effects of prevalence and bias on kappa must be considered when judging its magnitude. *(see* <http://ptjournal.apta.org/content/85/3/257> *for details on how to interpret the Kappa coefficient.)*

While the hold-out (test) data is not in general likely to be as accurate as our 87% training data result each time the model is applied, this model can be expected to correctly classify approximately 80% of all cases. And in our application of the model in the project quiz to th 20 test observations, the model was accurate on 18/20 cases (90%)


### Conclusions

The model shows that accuracy is greatly improved when methods of cross-validation are used. In particular, the random forest allows for thorough cross-validation with the training data, increasing the accuracy of the model significantly over that of the decision tree model w/o cross-validation.

